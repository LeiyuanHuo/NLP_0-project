#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar 11 10:00:09 2022

@author: yangjingjing
"""


#data
import pandas as pd

#load ciks to be looked up
df=pd.read_csv(r'/Users/./cik_count10616.csv')
cik=[]
for i in df['cik'].tolist():
    i=str(i)
    c='0'*(10-len(i))+i
    cik.append(c)


#find the submission info.
#submissions can be downloaded from https://www.sec.gov/edgar/sec-api-documentation
ac=[]
no=[]
for i in cik:
    try:
        path=r'/Users/./submissions/CIK'+i+'.json'
        with open(path, 'r') as f:
            f1=f.readlines()
            ac.append(f1[0])
    except:        
        no.append(i)



null=''


#save info. that urls need
cik_code=[]
tickers=[]
filingdates=[]
accn=[]
form=[]
reportdates=[]
primaryDocument=[]
for i in ac:
    try:
        a=eval(i)
        c=a['cik']
        t=a['tickers']
        
        f=a['filings']['recent']
        
        fd=f['filingDate']
        acc=f['accessionNumber']
        fo=f['form']
        rd=f['reportDate']
        pd=f['primaryDocument']
        for j in range(len(fd)):
            cik_code.append(c)

            tickers.append(t)

            filingdates.append(fd[j])

            accn.append(acc[j])

            form.append(fo[j])

            reportdates.append(rd[j])
            
            primaryDocument.append(pd[j])
    except:
        print(ac.index(i))              


import pandas as pd
data=pd.DataFrame()
data['cik']=cik_code
data['tic']=tickers
data['file_date']=filingdates
data['accn']=accn
data['form']=form
data['report_date']=reportdates
data['file_name']=primaryDocument

d1=data[(data['form']=='10-K')]
d1['accn']=d1['accn'].apply(lambda x:x.replace('-',''))
d1['cik']=d1['cik'].apply(lambda x: '0'*(10-len(x))+x)
d1.reset_index(drop=True)




#save urls
u=[]

for i in range(len(d1)):
    v1=d1.iloc[i]['cik']
    v2=d1.iloc[i]['accn']
    v3=d1.iloc[i]['file_name']
    v4=d1.iloc[i]['report_date'][:4]
    url='https://www.sec.gov/Archives/edgar/data/'+v1+'/'+v2+'/'+v3
    u.append(url)

d1['url']=u

#di.to_csv(...)










import time
#divide 每次爬虫的个数
d2=d1.iloc[:462]
u2=u[:462]

#462，1258,3646做不了 可以用api


#need to create files on the 1st tral
#import os
root_path=r'/Users/y./NLP_10616/'
#os.mkdir(r'/Users/./NLP_10616')

#ll=list(set(d1['cik'].tolist()))
#for i in ll:
#     path=root_path+i
#     os.mkdir(path)


#web scraping
start_time=time.time()
from selenium import webdriver
#from selenium.webdriver.support.wait import WebDriverWait
driver_path = r'/Users/./chromedriver'
browser = webdriver.Chrome(executable_path=driver_path)


 
unknown_problem=[]
total=len(d2)

for t in range(total):
    
    content=''
    lst=[]
    
    try:
        #get all contents from 10K htm
        browser.get(u2[t])
        c=browser.find_element_by_xpath('/html/body')
        content=c.text  
        lst=content.split('\n')
        
        #initialize
        fr=0
        start1=0
        start2=0
        end1=0
        end2=0
        
        
        l=len(lst)
        
        #remove 'Table of Contents' in every page
        for i in range(l):
            a=lst[i].strip().lower().replace(' ','')
            if a=='tableofcontents':
                lst[i]=''
        
        #locate 'item14' in catalog
        for i in range(l):
            a=lst[i].lower().replace(' ','')
            if a.startswith('item14'):
                fr=i
                break
        
        #sometimes 'item14' cannot be found
        if fr==0 or fr>700:
            print('no_found_from:',t)
            fr=100
        
        #locate 'item1'
        start1=fr      
        for i in range(fr+1,l):
            a=lst[i].lower().replace(' ','')
            if a.startswith('item1.') \
            or a.startswith('item1and') \
            or a.startswith('items1and') \
            or a.startswith('items1&') \
            or a.startswith('itemi.') \
            or a.startswith('item1–b') \
            or a.startswith('item1bu') \
            or a=='item1':
                start1=i
                break
        
        #locate 'item2'
        end1=start1
        for i in range(start1+1,l):  
            a=lst[i].lower().replace(' ','')
            if a.startswith('item2.') \
            or a.startswith('item3.') \
            or a.startswith('item2p') \
            or a.startswith('item2–p') \
            or a.startswith('item2-p') \
            or a=='item2':   
                end1=i
                break
        
        #extract contents between item1 and item2
        bis='\n'.join(lst[start1:end1])
        
        
        #locate 'item7'
        start2=end1
        for i in range(end1+1,l):
            a=lst[i].lower().replace(' ','')
            if a.startswith('item7.') \
            or a.startswith('item7m') \
            or a.startswith('item7–m') \
            or a.startswith('item7-m') \
            or a=='item7':
                start2=i
                break
        
        
        #locate 'item8'
        end2=start2
        for i in range(start2+1,l):
            a=lst[i].lower().replace(' ','')
            if a.startswith('item8.') \
            or a.startswith('item8f') \
            or a.startswith('item8–f') \
            or a.startswith('item8-f') \
            or a=='item8':
                end2=i
                break
        
        #extract contents between item7 and item8        
        comp='\n'.join(lst[start2:end2])
        
        #check stability
        print(t,':',fr,start1,end1,start2,end2)
        
        
        #save contents
        v1=d2.iloc[t]['cik']
        v2=d2.iloc[t]['file_date'][:4]
        path1=root_path+v1+'/data-'+v1+'-'+v2+'-10K-business.txt'
        path2=root_path+v1+'/data-'+v1+'-'+v2+'-10K-MD&A.txt'
        s1=open(path1,'w')
        s1.write(bis)
        s1.close()
        
        s2=open(path2,'w')
        s2.write(comp)
        s2.close()
        
        
    except:
        print('unknown_problem:',t)    
        unknown_problem.append(t)
        
        
        
print((time.time()-start_time)/60)        
browser.close()   
